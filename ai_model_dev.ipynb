{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-section",
   "metadata": {},
   "source": [
    "# Emissions Compliance Forecasting Model\n",
    "\n",
    "## Overview\n",
    "This notebook develops an AI/ML module to forecast emission rates (SO2, NOx, CO2) for power plant operations to ensure compliance with EPA 40 CFR Part 75. The module is designed for Rayfield Systems' energy compliance software to automate emissions monitoring and prevent regulatory violations.\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "**Energy Workflow**: Emissions compliance forecasting for power plant operations\n",
    "\n",
    "**Problem Statement**: Power plants must comply with EPA 40 CFR Part 75 emissions monitoring regulations. Predicting emission rates (SO2, NOx, CO2) automates compliance monitoring, addressing the pain point of manual regulatory checks.\n",
    "\n",
    "**Input Data**: Quarterly CEMS emissions data from EPA CAMPD, including:\n",
    "- Operational parameters: Gross Load (MWh), Heat Input (mmBtu), Sum of Operating Time\n",
    "- Categorical features: Primary Fuel Type, Unit Type, Quarter\n",
    "- Emissions: SO2 Mass, NOx Mass, CO2 Mass, and their rates (lbs/mmBtu or short tons/mmBtu)\n",
    "\n",
    "**Expected Output**: Predicted emission rates:\n",
    "- SO2 Rate (lbs/mmBtu)\n",
    "- NOx Rate (lbs/mmBtu)\n",
    "- CO2 Rate (short tons/mmBtu)\n",
    "\n",
    "**Baseline Model**: LinearRegression from scikit-learn for forecasting continuous emission rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "try:\n",
    "    df = pd.read_csv('quarterly-emissions-e59aa51f-1169-445d-918e-fb306c4f282a.csv')\n",
    "    print(f\"Dataset loaded successfully! Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset file not found. Please ensure the CSV file is in the working directory.\")\n",
    "    raise\n",
    "\n",
    "# Display basic data info\n",
    "print('\\nDataset Info:')\n",
    "print(df.info())\n",
    "print('\\nFirst 5 rows:')\n",
    "print(df.head())\n",
    "print('\\nDescriptive Statistics:')\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Analysis\n",
    "print(\"=== DATA QUALITY ANALYSIS ===\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Rows with zero operating time: {len(df[df['Sum of the Operating Time'] == 0])}\")\n",
    "print(f\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check target variable distributions\n",
    "targets = ['SO2 Rate (lbs/mmBtu)', 'NOx Rate (lbs/mmBtu)', 'CO2 Rate (short tons/mmBtu)']\n",
    "print(\"\\n=== TARGET VARIABLE ANALYSIS ===\")\n",
    "for target in targets:\n",
    "    print(f\"\\n{target}:\")\n",
    "    print(f\"  Mean: {df[target].mean():.4f}\")\n",
    "    print(f\"  Std: {df[target].std():.4f}\")\n",
    "    print(f\"  Min: {df[target].min():.4f}\")\n",
    "    print(f\"  Max: {df[target].max():.4f}\")\n",
    "    print(f\"  Missing: {df[target].isnull().sum()}\")\n",
    "\n",
    "# Check categorical variables\n",
    "print(\"\\n=== CATEGORICAL VARIABLES ===\")\n",
    "categorical_cols = ['Primary Fuel Type', 'Unit Type', 'Quarter']\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col} unique values: {df[col].nunique()}\")\n",
    "    print(df[col].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Data Preprocessing\n",
    "print(\"=== DATA PREPROCESSING ===\")\n",
    "\n",
    "# Create a copy for processing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Filter out zero operating time (these are non-operational periods)\n",
    "print(f\"Removing {len(df_processed[df_processed['Sum of the Operating Time'] == 0])} rows with zero operating time\")\n",
    "df_processed = df_processed[df_processed['Sum of the Operating Time'] > 0].copy()\n",
    "\n",
    "# Remove rows with missing target values\n",
    "for target in targets:\n",
    "    before = len(df_processed)\n",
    "    df_processed = df_processed.dropna(subset=[target])\n",
    "    after = len(df_processed)\n",
    "    if before != after:\n",
    "        print(f\"Removed {before - after} rows with missing {target}\")\n",
    "\n",
    "# Feature engineering with better logic\n",
    "print(\"\\nApplying feature engineering...\")\n",
    "\n",
    "# Only calculate rolling features if we have enough data per facility-unit\n",
    "facility_unit_counts = df_processed.groupby(['Facility ID', 'Unit ID']).size()\n",
    "valid_facility_units = facility_unit_counts[facility_unit_counts >= 4].index\n",
    "\n",
    "# Initialize rolling feature with NaN\n",
    "df_processed['Rolling_Heat_Input'] = np.nan\n",
    "\n",
    "# Calculate rolling average only for facility-units with enough data\n",
    "for (facility_id, unit_id) in valid_facility_units:\n",
    "    mask = (df_processed['Facility ID'] == facility_id) & (df_processed['Unit ID'] == unit_id)\n",
    "    df_processed.loc[mask, 'Rolling_Heat_Input'] = df_processed.loc[mask, 'Heat Input (mmBtu)'].rolling(4, min_periods=1).mean()\n",
    "\n",
    "# Fill remaining NaN values with current heat input\n",
    "df_processed['Rolling_Heat_Input'].fillna(df_processed['Heat Input (mmBtu)'], inplace=True)\n",
    "\n",
    "# Load to Heat Ratio with better handling\n",
    "df_processed['Load_to_Heat_Ratio'] = df_processed['Gross Load (MWh)'] / df_processed['Heat Input (mmBtu)'].replace(0, np.nan)\n",
    "df_processed['Load_to_Heat_Ratio'].fillna(df_processed['Load_to_Heat_Ratio'].median(), inplace=True)\n",
    "\n",
    "print(f\"Final dataset shape: {df_processed.shape}\")\n",
    "print(f\"Remaining missing values: {df_processed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Model Training with Cross-Validation\n",
    "print(\"=== MODEL TRAINING AND EVALUATION ===\")\n",
    "\n",
    "# Define features and targets\n",
    "features = ['Gross Load (MWh)', 'Heat Input (mmBtu)', 'Sum of the Operating Time', \n",
    "            'Rolling_Heat_Input', 'Load_to_Heat_Ratio', 'Primary Fuel Type', 'Unit Type', 'Quarter']\n",
    "targets = ['SO2 Rate (lbs/mmBtu)', 'NOx Rate (lbs/mmBtu)', 'CO2 Rate (short tons/mmBtu)']\n",
    "\n",
    "# Prepare data\n",
    "X = df_processed[features].copy()\n",
    "y = df_processed[targets].copy()\n",
    "\n",
    "# Handle any remaining missing values in features\n",
    "numerical_cols = ['Gross Load (MWh)', 'Heat Input (mmBtu)', 'Sum of the Operating Time', \n",
    "                  'Rolling_Heat_Input', 'Load_to_Heat_Ratio']\n",
    "categorical_cols = ['Primary Fuel Type', 'Unit Type', 'Quarter']\n",
    "\n",
    "for col in numerical_cols:\n",
    "    X[col].fillna(X[col].median(), inplace=True)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 'Unknown', inplace=True)\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Split data with stratification by quarter to ensure temporal balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=X['Quarter'])\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Train models for each target\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"\\nTraining model for {target}...\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1))\n",
    "    ])\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train[target], cv=5, scoring='r2')\n",
    "    \n",
    "    # Train final model\n",
    "    pipeline.fit(X_train, y_train[target])\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train[target], y_pred_train)\n",
    "    test_r2 = r2_score(y_test[target], y_pred_test)\n",
    "    test_mae = mean_absolute_error(y_test[target], y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test[target], y_pred_test))\n",
    "    \n",
    "    models[target] = pipeline\n",
    "    results[target] = {\n",
    "        'CV_R2_mean': cv_scores.mean(),\n",
    "        'CV_R2_std': cv_scores.std(),\n",
    "        'Train_R2': train_r2,\n",
    "        'Test_R2': test_r2,\n",
    "        'Test_MAE': test_mae,\n",
    "        'Test_RMSE': test_rmse\n",
    "    }\n",
    "    \n",
    "    print(f\"  CV R¬≤: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "    print(f\"  Train R¬≤: {train_r2:.4f}\")\n",
    "    print(f\"  Test R¬≤: {test_r2:.4f}\")\n",
    "    print(f\"  Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"  Test RMSE: {test_rmse:.4f}\")\n",
    "    \n",
    "    if test_r2 < 0:\n",
    "        print(f\"  WARNING: Negative R¬≤ indicates model performs worse than baseline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gridsearch-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV Implementation (as specified in project requirements)\n",
    "print(\"=== GRIDSEARCHCV HYPERPARAMETER TUNING ===\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for RandomForestRegressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [10, 15, 20],\n",
    "    'model__min_samples_split': [5, 10, 15],\n",
    "    'model__min_samples_leaf': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for each target\n",
    "gridsearch_results = {}\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"\\nPerforming GridSearchCV for {target}...\")\n",
    "    \n",
    "    # Create base pipeline\n",
    "    base_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "    ])\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        base_pipeline,\n",
    "        param_grid,\n",
    "        cv=3,  # 3-fold CV for faster execution\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "    \n",
    "    # Store results\n",
    "    gridsearch_results[target] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_estimator': grid_search.best_estimator_\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Best parameters for {target}:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"Best cross-validation R¬≤ score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_predictions = grid_search.best_estimator_.predict(X_test)\n",
    "    test_r2 = r2_score(y_test[target], test_predictions)\n",
    "    test_mae = mean_absolute_error(y_test[target], test_predictions)\n",
    "    \n",
    "    print(f\"Test set performance:\")\n",
    "    print(f\"  R¬≤ score: {test_r2:.4f}\")\n",
    "    print(f\"  MAE: {test_mae:.4f}\")\n",
    "    \n",
    "    # Compare with our current model\n",
    "    current_model_r2 = results[target]['Test_R2']\n",
    "    improvement = test_r2 - current_model_r2\n",
    "    print(f\"  Improvement over current model: {improvement:+.4f} R¬≤\")\n",
    "\n",
    "print(\"\\n=== GRIDSEARCHCV SUMMARY ===\")\n",
    "print(\"GridSearchCV systematically tested different hyperparameter combinations:\")\n",
    "print(\"- n_estimators: Number of trees in the forest\")\n",
    "print(\"- max_depth: Maximum depth of trees\")\n",
    "print(\"- min_samples_split: Minimum samples required to split a node\")\n",
    "print(\"- min_samples_leaf: Minimum samples required at a leaf node\")\n",
    "print(\"\\nThis automated tuning process helps find optimal parameters without manual trial-and-error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation Visualizations\n",
    "print(\"=== CREATING VISUALIZATIONS ===\")\n",
    "\n",
    "# Create subplots for predictions vs actual\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Predicted vs Actual Emission Rates', fontsize=16)\n",
    "\n",
    "for i, target in enumerate(targets):\n",
    "    y_pred = models[target].predict(X_test)\n",
    "    \n",
    "    axes[i].scatter(y_test[target], y_pred, alpha=0.5, s=20)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test[target].min(), y_pred.min())\n",
    "    max_val = max(y_test[target].max(), y_pred.max())\n",
    "    axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    \n",
    "    axes[i].set_xlabel(f'Actual {target}')\n",
    "    axes[i].set_ylabel(f'Predicted {target}')\n",
    "    axes[i].set_title(f'{target}\\nR¬≤ = {results[target][\"Test_R2\"]:.3f}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_predictions_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance analysis\n",
    "print(\"\\n=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "feature_importance_data = {}\n",
    "\n",
    "for target in targets:\n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = (numerical_cols + \n",
    "                    list(models[target].named_steps['preprocessor']\n",
    "                         .named_transformers_['cat']\n",
    "                         .get_feature_names_out(categorical_cols)))\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = models[target].named_steps['model'].feature_importances_\n",
    "    \n",
    "    # Create importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    feature_importance_data[target] = importance_df\n",
    "    \n",
    "    print(f\"\\nTop 10 features for {target}:\")\n",
    "    for idx, row in importance_df.head(10).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
    "fig.suptitle('Feature Importance by Target Variable', fontsize=16)\n",
    "\n",
    "for i, target in enumerate(targets):\n",
    "    top_features = feature_importance_data[target].head(10)\n",
    "    \n",
    "    axes[i].barh(range(len(top_features)), top_features['importance'])\n",
    "    axes[i].set_yticks(range(len(top_features)))\n",
    "    axes[i].set_yticklabels(top_features['feature'])\n",
    "    axes[i].set_xlabel('Importance')\n",
    "    axes[i].set_title(f'Top 10 Features for {target}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-diagnostics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Diagnostics and Recommendations\n",
    "print(\"=== MODEL DIAGNOSTICS AND RECOMMENDATIONS ===\")\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"\\n{target} Model Analysis:\")\n",
    "    \n",
    "    test_r2 = results[target]['Test_R2']\n",
    "    train_r2 = results[target]['Train_R2']\n",
    "    cv_r2 = results[target]['CV_R2_mean']\n",
    "    \n",
    "    # Overfitting check\n",
    "    overfitting = train_r2 - test_r2\n",
    "    if overfitting > 0.1:\n",
    "        print(f\"  ‚ö†Ô∏è  OVERFITTING DETECTED: Train R¬≤ ({train_r2:.3f}) >> Test R¬≤ ({test_r2:.3f})\")\n",
    "        print(f\"     Recommendation: Reduce model complexity, increase regularization\")\n",
    "    \n",
    "    # Performance assessment\n",
    "    if test_r2 < 0:\n",
    "        print(f\"  üö® CRITICAL: Model performs worse than baseline (R¬≤ = {test_r2:.3f})\")\n",
    "        print(f\"     Recommendation: Check data quality, feature engineering, or try different algorithms\")\n",
    "    elif test_r2 < 0.3:\n",
    "        print(f\"  ‚ö†Ô∏è  POOR PERFORMANCE: R¬≤ = {test_r2:.3f}\")\n",
    "        print(f\"     Recommendation: Improve feature engineering, collect more data, or try ensemble methods\")\n",
    "    elif test_r2 < 0.7:\n",
    "        print(f\"  ‚úÖ MODERATE PERFORMANCE: R¬≤ = {test_r2:.3f}\")\n",
    "        print(f\"     Recommendation: Fine-tune hyperparameters, add domain-specific features\")\n",
    "    else:\n",
    "        print(f\"  üéØ GOOD PERFORMANCE: R¬≤ = {test_r2:.3f}\")\n",
    "        print(f\"     Recommendation: Model ready for production with monitoring\")\n",
    "    \n",
    "    # Cross-validation consistency\n",
    "    cv_std = results[target]['CV_R2_std']\n",
    "    if cv_std > 0.1:\n",
    "        print(f\"  ‚ö†Ô∏è  HIGH VARIANCE: CV std = {cv_std:.3f}\")\n",
    "        print(f\"     Recommendation: Model performance is inconsistent across folds\")\n",
    "\n",
    "print(\"\\n=== OVERALL RECOMMENDATIONS ===\")\n",
    "print(\"1. SO2 Rate model needs significant improvement - investigate data quality\")\n",
    "print(\"2. Consider time-series specific models for temporal dependencies\")\n",
    "print(\"3. Add more domain-specific features (weather, maintenance schedules)\")\n",
    "print(\"4. Implement ensemble methods combining multiple algorithms\")\n",
    "print(\"5. Set up monitoring for model drift in production\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
